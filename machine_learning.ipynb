{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# sklearn imports\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# imblearn imports\n",
    "import imblearn.ensemble as ensemble\n",
    "import imblearn.metrics as imblearn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def increase_dummies(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def descending_sort_features(features, importances):\n",
    "    indices = np.argsort(importances)\n",
    "    \n",
    "    sorted_importances = importances[indices[::-1]]\n",
    "    sorted_features = features[indices[::-1]]\n",
    "    \n",
    "    result = np.dstack((sorted_features, sorted_importances))[0].tolist()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in saved csv\n",
    "\n",
    "raw_path = Path('data/raw_nyt_watson.csv')\n",
    "indices_path = Path('data/global_indicies.csv')\n",
    "\n",
    "raw_df = pd.read_csv(raw_path, parse_dates=True, index_col='Date')\n",
    "indices_df = pd.read_csv(indices_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set indicies_df date index\n",
    "\n",
    "indices_df[\"Date\"] = pd.to_datetime(indices_df[\"Date\"]).dt.date\n",
    "indices_df = indices_df.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot on Ticker\n",
    "\n",
    "indices_pivot = indices_df.pivot(columns = \"Ticker\", values = \"Close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill on any na in indices\n",
    "\n",
    "indices_pivot = indices_pivot.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns so its clear that these are closes\n",
    "\n",
    "d = {\n",
    "    'CAC' : 'CAC Close',\n",
    "    'FTSE' : 'FTSE Close',\n",
    "    'MDAX' : 'MDAX Close',\n",
    "    'SP500' : 'SP500 Close',\n",
    "    'TOPIX' : 'TOPIX Close'\n",
    "}\n",
    "\n",
    "indices_pivot = indices_pivot.rename(columns=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent change for each index\n",
    "\n",
    "indices_pivot.insert(0, \"TOPIX percent change\", indices_pivot['TOPIX Close'].pct_change() * 100)\n",
    "indices_pivot.insert(0, \"SP500 percent change\", indices_pivot['SP500 Close'].pct_change() * 100)\n",
    "indices_pivot.insert(0, \"MDAX percent change\", indices_pivot['MDAX Close'].pct_change() * 100)\n",
    "indices_pivot.insert(0, \"FTSE percent change\", indices_pivot['FTSE Close'].pct_change() * 100)\n",
    "indices_pivot.insert(0, \"CAC percent change\", indices_pivot['CAC Close'].pct_change() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of read-in features\n",
    "\n",
    "sentiment_df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneccesary information\n",
    "\n",
    "sentiment_df = sentiment_df.drop(columns=['Source', 'Headline', 'Lead Paragraph', 'URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies\n",
    "\n",
    "sentiment_df = pd.get_dummies(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by date and take the mean of the sentiment scores\n",
    "\n",
    "mean_sentiment_df = sentiment_df.groupby(['Date']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the features and target data\n",
    "\n",
    "mean_sentiment_df = pd.concat([indices_pivot.loc[:, \"CAC percent change\":\"TOPIX percent change\"], mean_sentiment_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create categories for the percent increase, 1 for increase, 0 for decrease/no change\n",
    "\n",
    "mean_sentiment_df.insert(0, \"TOPIX increase\", mean_sentiment_df['TOPIX percent change'].apply(increase_dummies))\n",
    "mean_sentiment_df.insert(0, \"SP500 increase\", mean_sentiment_df['SP500 percent change'].apply(increase_dummies))\n",
    "mean_sentiment_df.insert(0, \"MDAX increase\", mean_sentiment_df['MDAX percent change'].apply(increase_dummies))\n",
    "mean_sentiment_df.insert(0, \"FTSE increase\", mean_sentiment_df['FTSE percent change'].apply(increase_dummies))\n",
    "mean_sentiment_df.insert(0, \"CAC increase\", mean_sentiment_df['CAC percent change'].apply(increase_dummies))\n",
    "\n",
    "# create categories for the sentiment, 1 for positive, 0 for negative/neutral\n",
    "mean_sentiment_df['Headline Sentiment'] = mean_sentiment_df[\"Headline Sentiment\"].apply(increase_dummies)\n",
    "mean_sentiment_df['Lead Paragraph Sentiment'] = mean_sentiment_df[\"Lead Paragraph Sentiment\"].apply(increase_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "\n",
    "mean_sentiment_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_columns = [\"CAC percent change\", \"FTSE percent change\", \"MDAX percent change\", \"SP500 percent change\", \"TOPIX percent change\"]\n",
    "inc_columns = [\"CAC increase\", \"FTSE increase\", \"MDAX increase\", \"SP500 increase\", \"TOPIX increase\"]\n",
    "\n",
    "# create dataframe for BalancedRandomForest\n",
    "brf_df = mean_sentiment_df.drop(pct_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [i for i in brf_df.columns if i not in inc_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = brf_df[x_cols]\n",
    "\n",
    "brf_indices = {\n",
    "    \"CAC\" : {},\n",
    "    \"FTSE\" : {}, \n",
    "    \"MDAX\" : {}, \n",
    "    \"SP500\" : {}, \n",
    "    \"TOPIX\" : {},\n",
    "}\n",
    "\n",
    "brf_features = {}\n",
    "\n",
    "for key in brf_indices.keys():\n",
    "    brf_indices[key][\"target\"] = brf_df[key + \" increase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler, split percentage, and random state\n",
    "scaler = StandardScaler()\n",
    "train_size = 0.7\n",
    "rs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 1\n",
      "test 2\n",
      "test 3\n"
     ]
    }
   ],
   "source": [
    "brf_features[\"X train\"], brf_features[\"X test\"] = train_test_split(X, train_size=train_size, random_state=rs)\n",
    "\n",
    "scaler.fit(brf_features[\"X train\"])\n",
    "\n",
    "brf_features[\"X train scaled\"] = scaler.transform(brf_features[\"X train\"])\n",
    "brf_features[\"X test scaled\"] = scaler.transform(brf_features[\"X test\"])\n",
    "\n",
    "brf = ensemble.BalancedRandomForestClassifier(n_estimators=10000, random_state=rs)\n",
    "\n",
    "for key, brf_data in brf_indices.items():\n",
    "    \n",
    "    print(\"test 1\")\n",
    "    \n",
    "    brf_data[\"y train\"], brf_data[\"y test\"] = train_test_split(brf_data[\"target\"], train_size=train_size, random_state=rs)\n",
    "    print(\"test 2\")\n",
    "    brf.fit(brf_features[\"X train scaled\"], brf_data[\"y train\"])\n",
    "    \n",
    "    print(\"test 3\")\n",
    "    \n",
    "    brf_data[\"predictions\"] = brf.predict(brf_features[\"X test scaled\"])\n",
    "    \n",
    "    brf_data[\"Accuracy\"] = metrics.balanced_accuracy_score(brf_data[\"y test\"], brf_data[\"predictions\"]) \n",
    "    \n",
    "    brf_data[\"Confusion Matrix\"] = metrics.confusion_matrix(brf_data[\"y test\"], brf_data[\"predictions\"])\n",
    "    \n",
    "    brf_data[\"Classification Report\"] = imblearn_metrics.classification_report_imbalanced(\n",
    "        brf_data[\"y test\"], \n",
    "        brf_data[\"predictions\"], \n",
    "        digits=4\n",
    "    )\n",
    "    \n",
    "    brf_data[\"Feature Importance\"] = descending_sort_features(\n",
    "        brf_features[\"X train\"].columns, \n",
    "        brf.feature_importances_\n",
    "    )\n",
    "    \n",
    "del brf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAC Accuracy: 0.5343\n",
      "\n",
      "CAC Confusion Matrix: \n",
      "[[22 13]\n",
      " [28 22]]\n",
      "\n",
      "CAC Classification Report: \n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0     0.4400    0.6286    0.4400    0.5176    0.5259    0.2818        35\n",
      "          1     0.6286    0.4400    0.6286    0.5176    0.5259    0.2714        50\n",
      "\n",
      "avg / total     0.5509    0.5176    0.5509    0.5176    0.5259    0.2757        85\n",
      "\n",
      "\n",
      "CAC Top Five features\n",
      "Headline Anger: 0.0853\n",
      "Lead Paragraph Sadness: 0.0829\n",
      "Headline Joy: 0.0814\n",
      "Lead Paragraph Anger: 0.0793\n",
      "Lead Paragraph Fear: 0.0769\n",
      "\n",
      "\n",
      "FTSE Accuracy: 0.4338\n",
      "\n",
      "FTSE Confusion Matrix: \n",
      "[[19 18]\n",
      " [31 17]]\n",
      "\n",
      "FTSE Classification Report: \n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0     0.3800    0.5135    0.3542    0.4368    0.4265    0.1848        37\n",
      "          1     0.4857    0.3542    0.5135    0.4096    0.4265    0.1790        48\n",
      "\n",
      "avg / total     0.4397    0.4235    0.4442    0.4215    0.4265    0.1815        85\n",
      "\n",
      "\n",
      "FTSE Top Five features\n",
      "Headline Disgust: 0.1003\n",
      "Lead Paragraph Sadness: 0.0879\n",
      "Lead Paragraph Joy: 0.0808\n",
      "Headline Fear: 0.0804\n",
      "Section_Business: 0.0755\n",
      "\n",
      "\n",
      "MDAX Accuracy: 0.4804\n",
      "\n",
      "MDAX Confusion Matrix: \n",
      "[[16 18]\n",
      " [26 25]]\n",
      "\n",
      "MDAX Classification Report: \n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0     0.3810    0.4706    0.4902    0.4211    0.4803    0.2302        34\n",
      "          1     0.5814    0.4902    0.4706    0.5319    0.4803    0.2311        51\n",
      "\n",
      "avg / total     0.5012    0.4824    0.4784    0.4876    0.4803    0.2308        85\n",
      "\n",
      "\n",
      "MDAX Top Five features\n",
      "Section_Business: 0.0910\n",
      "Headline Disgust: 0.0903\n",
      "Headline Anger: 0.0884\n",
      "Headline Joy: 0.0864\n",
      "Lead Paragraph Anger: 0.0830\n",
      "\n",
      "\n",
      "SP500 Accuracy: 0.5300\n",
      "\n",
      "SP500 Confusion Matrix: \n",
      "[[21 14]\n",
      " [27 23]]\n",
      "\n",
      "SP500 Classification Report: \n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0     0.4375    0.6000    0.4600    0.5060    0.5254    0.2799        35\n",
      "          1     0.6216    0.4600    0.6000    0.5287    0.5254    0.2721        50\n",
      "\n",
      "avg / total     0.5458    0.5176    0.5424    0.5194    0.5254    0.2753        85\n",
      "\n",
      "\n",
      "SP500 Top Five features\n",
      "Lead Paragraph Sadness: 0.0886\n",
      "Headline Disgust: 0.0854\n",
      "Lead Paragraph Disgust: 0.0830\n",
      "Headline Sadness: 0.0811\n",
      "Lead Paragraph Anger: 0.0767\n",
      "\n",
      "\n",
      "TOPIX Accuracy: 0.4476\n",
      "\n",
      "TOPIX Confusion Matrix: \n",
      "[[19 25]\n",
      " [22 19]]\n",
      "\n",
      "TOPIX Classification Report: \n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0     0.4634    0.4318    0.4634    0.4471    0.4473    0.1995        44\n",
      "          1     0.4318    0.4634    0.4318    0.4471    0.4473    0.2007        41\n",
      "\n",
      "avg / total     0.4482    0.4471    0.4482    0.4471    0.4473    0.2001        85\n",
      "\n",
      "\n",
      "TOPIX Top Five features\n",
      "Section_Foreign: 0.0884\n",
      "Headline Joy: 0.0826\n",
      "Lead Paragraph Fear: 0.0798\n",
      "Lead Paragraph Sadness: 0.0794\n",
      "Headline Anger: 0.0789\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in brf_indices.items():\n",
    "    print(f\"{key} Accuracy: {value['Accuracy']:.4f}\\n\")\n",
    "    print(f\"{key} Confusion Matrix: \\n{value['Confusion Matrix']}\\n\")\n",
    "    print(f\"{key} Classification Report: \\n{value['Classification Report']}\\n\")\n",
    "    print(f\"{key} Top Five features\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(f\"{value['Feature Importance'][i][0]}: {value['Feature Importance'][i][1]:.4f}\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project2env] *",
   "language": "python",
   "name": "conda-env-project2env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
